---
title: "Data from Facebook Confessions Forums"
author: "Adrian Naranjo"
---

<br>

# Part 1: Introduction & Descriptive Analysis

#### Confession pages are...

* Social networking websites
* Used at schools and universities 
* Students post thoughts, questions, secrets to the community
* Anonymous


```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Load libraries
library(tidyverse)
library(lubridate)
library(tokenizers)
library(tm)
library(knitr)
library(ggthemes)
library(tidytext)
library(topicmodels)
library(ldatuning)
library(kableExtra)
library(gridExtra)
library(cluster)
library(fpc)
library(xgboost)
library(randomForest)
```

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# Import data
us_data <- read_csv('data/us.csv') %>% 
  mutate(country = 'united_states')

uk_data <- read_csv('data/uk.csv') %>% 
  mutate(country = 'united_kingdom')

canada_data <- read_csv('data/canada.csv') %>% 
  mutate(country = 'canada')

i_words <- c("\\s*i\\s+", "i've", "i'm", "i'll", "i'd")
u_words <- c("you\\s", "you've", "you're", "you'll", "you'd", "\\su\\s", "u've", "u're", "u'll", "u'd")

i_words <- str_c(i_words, collapse = "|")
u_words <- str_c(u_words, collapse = "|")

combined_data <- bind_rows(us_data, uk_data, canada_data) %>% 
  rename('id' = 'X1') %>% 
  mutate(id = as.integer(seq(1, dim(.)[1], 1))
         ,year = year(time)
         ,month = month(time)
         ,day = day(time)
         ,hour = hour(time)
         ,message = str_to_lower(message)
         ,num_words = lengths(gregexpr("\\W+", message))
         ,characters_per_word = str_length(message) / num_words
         ,i_count = str_count(message, i_words)
         ,i_perc = str_count(message, i_words) / num_words 
         ,u_count = str_count(message, u_words)
         ,u_perc = str_count(message, u_words) / num_words
         ,iu_ratio = i_count / (u_count + 1)
         ,stopword_count = str_count(message, stopwords())
         ,stopword_perc = str_count(message, stopwords()) / num_words) %>% 
  select(-post_id, -time)

combined_data$university[combined_data$university == "Purdue University--West Lafayette:574281165929300posts"] <- "Purdue University"
combined_data$university[combined_data$university == "Rhodes College:418059621625397posts"] <- "Rhodes College"
combined_data$university[combined_data$university == "Rutgers, the State University of NJ:599429153419231posts"] <- "Rutgers University"
combined_data$university[combined_data$university == "Syracuse University:206226116186911posts"] <- "Syracuse University"
combined_data$university[combined_data$university == "Rice University:266887743442106posts"] <- "Rice University"
combined_data$university[combined_data$university == "Rensselaer Polytechnic Institute:168590540005034posts"] <- "Rensselaer Polytechnic Institute"
combined_data$university[combined_data$university == "Pepperdine University:1426040870946133posts"] <- "Pepperdine University"
combined_data$university[combined_data$university == "Pepperdine University:480574488664604posts"] <- "Pepperdine University"
combined_data$university[combined_data$university == "Pennsylvania State University--University Park:479597885433626posts"] <- "Pennsylvania State University--University Park"
combined_data$university[combined_data$university == "Pepperdine University:480574488664604posts"] <- "Pepperdine University"
combined_data$university[combined_data$university == "Princeton:423593751057693posts"] <- "Princeton"
combined_data$university[combined_data$university == "Smith College:656392224417505posts"] <- "Smith College"
combined_data$university[combined_data$university == "Southwestern University:154639111371519posts"] <- "Southwestern University"
combined_data$university[combined_data$university == "St. Lawrence University:578380385537855posts"] <- "St. Lawrence University"
```

<br>

### How much data?

```{r, message=FALSE, echo=FALSE, warning=FALSE}
tibble(country = c('United States', 'Canada', 'United Kingdom')
      ,num_observations = c(nrow(us_data), nrow(canada_data), nrow(uk_data))) %>% 
    kable(align = c('c', 'c'), format = 'html') %>% 
  kable_styling(full_width = F, position = "left")
```

<br>

### Date Range (Year)

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 6, fig.width = 6, fig.align = "center"}
combined_data %>% 
  group_by(year) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% 
  head(10) %>% 
  ggplot(aes(x = year, y = count)) +
  geom_bar(stat = "identity", fill = 'black') +
  theme_fivethirtyeight()
```

<br>

### Universities with the Most Posts

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 6, fig.width = 6, fig.align = "center"}
total_university_posts <- combined_data %>% 
  group_by(university) %>% 
  summarise(count = n()) %>% 
  arrange(desc(count)) %>% 
  filter(count > 100)
  
total_university_posts %>% 
  head(10) %>% 
  ggplot(aes(x = reorder(university, count), y = count)) +
  geom_bar(stat = "identity", fill = 'black') +
  coord_flip() +
  theme_fivethirtyeight()
```

<br>

### Range of Posts for the Universities

```{r, message=FALSE, echo=FALSE, warning=FALSE}
for_quantiles <- combined_data %>% 
  group_by(university) %>% 
  summarise(count = n())

quantile(for_quantiles$count)
```

<br>

### What [raw] variables?

* Datetime
* Post
* Number of Likes
* Number of Comments
* University
* Country

<br>

### Variables have we added?

* Separated Datetime into: Year, month, day
* Number of Words
* Number of Characters
* Characters per Word
* I-Word Count 
* I-Word Percent of Total Words
* You-Word Count
* You-Word Percent of Total Words
* I-Word to U-Word Ratio
* Stop Word Count
* Stop Word Percent of Total Words

**I-Words** = I, I've, I'd, I'll, I'm

<br>

**You-Words** = You, You've, You'd, You'll, You're

```{r, message=FALSE, echo=FALSE, warning=FALSE}
# I-Words versus You-Words
for_iu_words <- combined_data %>%
  filter(university %in% total_university_posts$university) %>% 
  group_by(university) %>% 
  summarise(i_words = sum(i_count)
            ,u_words = sum(u_count)
            ,i_to_u_ratio = round(i_words / u_words, 2)) 


for_iu_good <- for_iu_words %>% 
  filter(i_words > 0 & u_words > 0) %>% 
  arrange(i_to_u_ratio) %>% 
  select(university, i_to_u_ratio) %>% 
  head(3) %>% 
  gather()

for_iu_bad <- for_iu_words %>% 
  filter(i_words > 0 & u_words > 0) %>% 
  arrange(desc(i_to_u_ratio)) %>% 
  select(university, i_to_u_ratio) %>% 
  head(3) %>% 
  gather()


universities_iu_good <- filter(for_iu_good, key == 'university')$value
universities_iu_bad <- filter(for_iu_bad, key == 'university')$value


graph_summary <- for_iu_good %>%
  mutate(university = rep(universities_iu_good, nrow(.)/length(universities_iu_good))) %>%
  filter(key %in% c('i_to_u_ratio'))

graph_summary2 <- for_iu_bad %>%
  mutate(university = rep(universities_iu_bad, nrow(.)/length(universities_iu_bad))) %>%
  filter(key %in% c('i_to_u_ratio'))
```

<br>

### Most Narcissistic Schools

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 6, fig.width = 6, fig.align = "center"}
ggplot(graph_summary2, aes(x = reorder(key, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  facet_wrap(~ university) +
  ggtitle("Number of 'I-words' vs Number of 'U-words'") +
  theme_fivethirtyeight()
```

<br>

### Most Altruistic Schools

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 6, fig.width = 6, fig.align = "center"}
ggplot(graph_summary, aes(x = reorder(key, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  facet_wrap(~ university) +
  ggtitle("Number of 'I-words' vs Number of 'U-words'") +
  theme_fivethirtyeight()
```

<br>

### Average Number of Words

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 6, fig.width = 6, fig.align = "center"}
combined_data %>% 
  filter(university %in% total_university_posts$university) %>%
  group_by(university) %>% 
  summarise(avg_words = mean(num_words)) %>% 
  head(10) %>% 
  ggplot(aes(x = reorder(university, avg_words), y = avg_words)) +
  geom_bar(stat = "identity", fill = 'black') +
  coord_flip() +
  theme_fivethirtyeight()
```

<br>

### Average Characters per Word

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 6, fig.width = 6, fig.align = "center"}
combined_data %>% 
  filter(university %in% total_university_posts$university) %>%
  group_by(university) %>% 
  summarise(avg_chars = mean(characters_per_word)) %>% 
  head(10) %>% 
  ggplot(aes(x = reorder(university, avg_chars), y = avg_chars)) +
  geom_bar(stat = "identity", fill = 'black') +
  coord_flip() +
  theme_fivethirtyeight()
```

<br>

# Part 2: Sentiment Analysis

```{r, message=FALSE, echo=FALSE, warning=FALSE}
dfCorpus <- VCorpus(VectorSource(combined_data$message))

dfCorpus2 <- tm_map(dfCorpus, stripWhitespace)
dfCorpus2 <- tm_map(dfCorpus2, removePunctuation)
dfCorpus2 <- tm_map(dfCorpus2, removeNumbers)
dfCorpus2 <- tm_map(dfCorpus2, content_transformer(tolower))
dfCorpus2 <- tm_map(dfCorpus2, removeWords, stopwords("english"))
dfCorpus2 <- tm_map(dfCorpus2, stemDocument)

myStopwords <- c('can', 'say', 'one', 'way', 'use', 'also', 'howev', 'tell', 'will', 'much'
                 ,'tend', 'even', 'like', 'particular', 'rather', 'said', 'get', 'well', 'make', 'ask', 'come'
                 ,'end', 'first', 'two', 'often', 'may', 'might', 'see', 'someth', 'thing', 'point'
                 ,'post', 'think', 'anoth', 'put', 'set', 'want', 'sure', 'kind'
                 ,'yes', 'etc',  'sinc', 'attempt', 'lack', 'seen', 'awar', 'littl', 'ever'
                 , 'what', 'just', 'that', 'let', 'dont', 'cant', 'still'
                 , 'ill', 'ive', 'yall')

dfCorpus2 <- tm_map(dfCorpus2, removeWords, myStopwords)
dtm <- DocumentTermMatrix(dfCorpus2)
dtm2 <- removeSparseTerms(dtm, 0.99)

rowTotals <- apply(dtm2, 1, sum)
dtm3 <- dtm2[rowTotals > 0,]

########## Sentiment Dictionaries #############
afinn_dict <- get_sentiments("afinn") %>% rename('afinn' = 'score')

bing_dict <- get_sentiments("bing") %>% 
  rename('bing' = 'sentiment') %>% 
  mutate(bing = ifelse(bing == 'negative', 0, ifelse(bing == 'positive', 1, NA))
         ,bing = as.integer(bing))

nrc_dict <- get_sentiments("nrc") %>% rename('nrc' = 'sentiment')


########## Sentiment Analysis Data Structures #################
tidy_dtm <- tidy(dtm3)
tidy_dtm <- left_join(tidy_dtm, bing_dict, by = c('term' = 'word'))
tidy_dtm <- left_join(tidy_dtm, afinn_dict, by = c('term' = 'word'))
tidy_dtm <- left_join(tidy_dtm, nrc_dict, by = c('term' = 'word'))

tidy_dtm <- tidy_dtm %>% 
  mutate(nrc = ifelse(is.na(nrc), 'neutral', nrc)
         ,afinn = ifelse(is.na(afinn), 0, afinn)
         ,bing = ifelse(is.na(bing), 99, bing)
         ,document = as.integer(document))

tidy_dtm <- left_join(tidy_dtm, select(combined_data, -message), by = c('document' = 'id'))

tidy_dtm_nrc <- tidy_dtm %>% 
  select(document, nrc) %>% 
  group_by(document, nrc) %>% 
  summarise('count' = n()) %>% 
  spread(nrc, count, fill = 0)

tidy_dtm_bing <- tidy_dtm %>% 
  filter(bing != 99) %>% 
  group_by(document) %>% 
  summarise(avg_bing = mean(bing))

tidy_dtm_afinn <- tidy_dtm %>% 
  filter(afinn != 0) %>% 
  group_by(document) %>% 
  summarise(avg_afinn = mean(afinn))


tidy_dtm_grouped <- tidy_dtm %>% 
  select(document, num_likes, num_comments, university, country, year, month, day, hour, num_words, i_count, u_count
         , i_perc, u_perc, stopword_count, stopword_perc, iu_ratio) %>% 
  unique()

tidy_dtm_grouped <- left_join(tidy_dtm_grouped, tidy_dtm_nrc, by = c('document' = 'document'))
tidy_dtm_grouped <- left_join(tidy_dtm_grouped, tidy_dtm_afinn, by = c('document' = 'document'))
tidy_dtm_grouped <- left_join(tidy_dtm_grouped, tidy_dtm_bing, by = c('document' = 'document'))
```

<br>

Preprocessing included:

1. Removal of stopwords, punctuation, whitespace 
2. Removal of words with a sparsity greater than 99%
    * Led to reduction of words from 42,961 words to 393 words
3. Added 3 sentiment libraries: afinn, bing, and nrc
    * We'll focus on nrc, but afinn and bing are left in for use in unsurpervised learning
    
<br>

### Most Used Words
    
```{r, message=FALSE, echo=FALSE, warning=FALSE}
tidy(dtm3) %>% 
  group_by(term) %>% 
  summarise(count = sum(count)) %>% 
  mutate(perc = round(count / dim(tidy(dtm3))[1], 4)) %>% 
  arrange(desc(count)) %>% 
  head(10) %>% 
  kable(align = c('c', 'c', 'c'), format = 'html') %>% 
  kable_styling(full_width = F, position = "left")
```

<br>

### Sentiments

* Anger
* Anticipation
* Disgust
* Fear
* Negative
* Positive
* Joy
* Sadness
* Surprise
* Trust

```{r, message=FALSE, echo=FALSE, warning=FALSE}
graph_summary3 <- tidy_dtm_grouped %>% 
  filter(university %in% total_university_posts$university) %>% 
  select(-num_likes, -num_comments, -country, -i_count, -u_count, -year, -month, -day, -iu_ratio,
           -num_words, -avg_afinn, -avg_bing, -document, -hour, -i_perc, -u_perc, -stopword_count, -stopword_perc) %>% 
  group_by(university) %>% 
  summarise(anger = log(mean(anger) + 1)
            ,anticipation = log(mean(anticipation) + 1)
            ,disgust = log(mean(disgust) + 1)
            ,fear = log(mean(fear) + 1)
            ,joy = log(mean(joy) + 1)
            ,negative = log(mean(negative) + 1)
            ,positive = log(mean(positive) + 1)
            ,sadness = log(mean(sadness) + 1)
            ,surprise = log(mean(surprise) + 1)
            ,trust = log(mean(trust) + 1)) 

graph_summary3 <- graph_summary3 %>% gather()
universities <- filter(graph_summary3, key == 'university')$value

graph_summary4 <- graph_summary3 %>% 
  mutate(university = rep(universities, nrow(.)/length(universities))) %>% 
  filter(!key == 'university' | value == 0) %>% 
  group_by(key) %>% 
  mutate(rank = dense_rank(desc(value))) %>% 
  filter(!rank > 10)

graph_data_anger <- filter(graph_summary4, key == 'anger') %>% mutate(value = round(as.double(value), 3))
graph_data_anticipation <- filter(graph_summary4, key == 'anticipation') %>% mutate(value = round(as.double(value), 3))
graph_data_disgust <- filter(graph_summary4, key == 'disgust') %>% mutate(value = round(as.double(value), 3))
graph_data_fear <- filter(graph_summary4, key == 'fear') %>% mutate(value = round(as.double(value), 3))
graph_data_joy <- filter(graph_summary4, key == 'joy') %>% mutate(value = round(as.double(value), 3))
graph_data_negative <- filter(graph_summary4, key == 'negative') %>% mutate(value = round(as.double(value), 3))
graph_data_positive <- filter(graph_summary4, key == 'positive') %>% mutate(value = round(as.double(value), 3))
graph_data_sadness <- filter(graph_summary4, key == 'sadness') %>% mutate(value = round(as.double(value), 3))
graph_data_surprise <- filter(graph_summary4, key == 'surprise') %>% mutate(value = round(as.double(value), 3))
graph_data_trust <- filter(graph_summary4, key == 'trust') %>% mutate(value = round(as.double(value), 3))

plot_anger <- ggplot(graph_data_anger, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_anticipation <- ggplot(graph_data_anticipation, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_disgust <- ggplot(graph_data_disgust, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_fear <- ggplot(graph_data_fear, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_joy <- ggplot(graph_data_joy, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_negative <- ggplot(graph_data_negative, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_positive <- ggplot(graph_data_positive, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_sadness <- ggplot(graph_data_sadness, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_surprise <- ggplot(graph_data_surprise, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()

plot_trust <- ggplot(graph_data_trust, aes(x = reorder(university, value), y = value)) +
  geom_bar(stat = 'identity', fill = 'black') +
  coord_flip() +
  labs(x = 'university', y = 'value') +
  theme_economist()
```

<br>

### Negative versus Positive Sentiments

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
grid.arrange(plot_negative, plot_positive, nrow = 1, ncol = 2)
```

<br>

### Misc Negative Sentiments

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
grid.arrange(plot_anger, plot_sadness, plot_fear, plot_disgust, nrow = 2, ncol = 2)
```

<br> 

### Misc Positive Sentiments

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
grid.arrange(plot_anticipation, plot_joy, plot_surprise, plot_trust, nrow = 2, ncol = 2)
```

<br>

# Part 3: Prediction

<br>

### Boosted Tree

* ETA (Learning Rate) = .3
* Max Depth = 6
* Grow Policy = Depth-wise
* Used 10-Fold Cross-Validation

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
number_of_classes <- length(unique(tidy_dtm_grouped$country))

tidy_dtm_grouped$country <- factor(ifelse(tidy_dtm_grouped$country == 'united_states', 1
                                          , ifelse(tidy_dtm_grouped$country == 'canada', 2, 0))) 

training_data <- sample_frac(drop_na(tidy_dtm_grouped), .8)
validation_data <- anti_join(drop_na(tidy_dtm_grouped), training_data)

x <- model.matrix(~ ., data = select(training_data, -country, -university, -document, -year, -month, -day, -hour))[,-1]
y <- select(training_data, country) %>% as.matrix()

dtrain <- xgb.DMatrix(data = x, label = y)

x_test <- model.matrix(~ ., data = select(validation_data, -country, -university, -document, -year, -month, -day, -hour))[,-1]

xgboost_cv <- xgb.cv(data = dtrain
                     , nrounds = 10000
                     , nfold = 10
                     , early_stopping_rounds = 10
                     , num_class = number_of_classes
                     , objective = "multi:softmax"
                     ,verbose = FALSE)

xgboost_model <- xgboost(data = dtrain
                         ,objective = "multi:softmax"
                         ,num_class = number_of_classes
                         ,nrounds = xgboost_cv$best_iteration
                         ,verbose = FALSE)

validation_data_xgb <- validation_data %>% 
  mutate(prediction = predict(xgboost_model, x_test)
         ,accuracy = ifelse(country == prediction, 1, 0))

tibble('Model' = 'Boosted Tree', 'Accuracy' = mean(validation_data_xgb$accuracy)) %>% 
  kable(align = c('c'), format = 'html') %>% 
  kable_styling(full_width = F, position = "left")
```

<br>

### Random Forest

```{r, message=FALSE, echo=FALSE, warning=FALSE}
rf_model <- randomForest(country ~ ., data = select(training_data, -university, -document, -year, -month, -day, -hour))
        
validation_data_rf <- validation_data %>% 
  mutate(prediction = predict(rf_model, validation_data)
         ,accuracy = ifelse(country == prediction, 1, 0))

tibble('Model' = 'Random Forest', 'Accuracy' = mean(validation_data_rf$accuracy)) %>%
  kable(align = c('c'), format = 'html') %>% 
  kable_styling(full_width = F, position = "left")
```

<br>

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
varImpPlot(rf_model)
```

<br>

### Bagging

```{r, message=FALSE, echo=FALSE, warning=FALSE}
bag_model <- randomForest(country ~ .
                          , data = select(training_data, -university, -document, -year, -month, -day, -hour)
                          , mtry = dim(select(training_data, -university, -document, -year, -month, -day, -hour))[2] - 1)

validation_data_bag <- validation_data %>% 
  mutate(prediction = predict(bag_model, validation_data)
         ,accuracy = ifelse(country == prediction, 1, 0))

tibble('Model' = 'Bagging', 'Accuracy' = mean(validation_data_rf$accuracy)) %>%
  kable(align = c('c'), format = 'html') %>% 
  kable_styling(full_width = F, position = "left")
```


<br>

# Part 4: Unsupervised Learning

<br>

### Clustering: How many Clusters (where's the elbow)?

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
tidy_dtm_grouped_clust <- select(tidy_dtm_grouped, -country, -university, -document, -year, -month, -day, -hour)
scaled_data <- scale(model.matrix(~ ., data = drop_na(tidy_dtm_grouped_clust))[,-1])

kmeans_out <- sapply(1:15,
                     function(k){kmeans(scaled_data, k, nstart = 50, iter.max = 100)$tot.withinss})

plot_kmeans <- plot(1:15, kmeans_out
     , type = "b"
     , pch = 19
     , frame = FALSE
     , xlab="Number of clusters K"
     , ylab="Total within-clusters sum of squares")
```

<br>

### We'll go with 13...

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
kmean_obj <- kmeans(scaled_data, 13, nstart = 50, iter.max = 100)

plotcluster(drop_na(tidy_dtm_grouped_clust), kmean_obj$cluster)
```

<br>

### Principal Components: What matters?

```{r, message=FALSE, echo=FALSE, warning=FALSE}
pca_results <- princomp(scaled_data)
summary(pca_results) 
```

<br>

### Plotting the First Two Principal Components?

```{r, message=FALSE, echo=FALSE, warning=FALSE, fig.height = 8, fig.width = 8, fig.align = "center"}
biplot(pca_results)
```


# Next Steps

* Investigate Canada
* Predict more granular class (e.g. region, state, university)
* Figure out why we're getting 100% accuracy (this seems unrealistic)
* Spend more time on the unsupervised learning
* Topic Modeling